db <- array(0,19)
for (k in 2:20){
print(k)
cn <- myKmean(X1,k)$cluster
db[k-1]<- index.DB(X1, cn)$DB
}
print(db)
plot(2:20, db)
sil <- array(0,19)
for (k in 2:20){
print(k)
#cn <- consensusCLustering_Boot(X,k,clustering = myKmean, C=100)
cn <- myEM(X1,k)$cluster
sil[k-1]<- mean(silhouette(cn, dist(X1))[,3])
}
print(sil)
plot(2:20, sil)
sil <- array(0,19)
for (k in 2:20){
print(k)
#cn <- consensusCLustering_Boot(X,k,clustering = myKmean, C=100)
cn <- myHierarchicalClustering(X1,k)$cluster
sil[k-1]<- mean(silhouette(cn, dist(X1))[,3])
}
print(sil)
plot(2:20, sil)
sil <- array(0,19)
for (k in 2:20){
print(k)
#cn <- consensusCLustering_Boot(X,k,clustering = myKmean, C=100)
cn <- myHierarchicalClustering(X1,k,linkage = "average")$cluster
sil[k-1]<- mean(silhouette(cn, dist(X1))[,3])
}
print(sil)
plot(2:20, sil)
db <- array(0,19)
for (k in 2:20){
print(k)
cn <- myEM(X1,k)$cluster
db[k-1]<- index.DB(X1, cn)$DB
}
print(db)
plot(2:20, db)
db <- array(0,19)
for (k in 2:20){
print(k)
cn <- myHierarchicalClustering(X1,k)$cluster
db[k-1]<- index.DB(X1, cn)$DB
}
print(db)
plot(2:20, db)
kolesnikov(X,20)
kolesnikov(X,20, clusterAlg = myHierarchicalClustering)
kolesnikov(X,20, clusterAlg = myEM)
myEM <- function(X,k, ...){
em <- Mclust(X,k, verbose = F, ...)
prediction <- function(x){
predict.Mclust(em, x)$classification
}
return(list("cluster"=em$classification, "centers"=t(em$parameters$mean), "predict"= prediction))
}
kolesnikov(X,20, clusterAlg = myEM)
apply(X,2,mean)
Outliers
X <- wine[2:14]
dim(X)
Outliers <- c()
vars <- 1:ncol(X)
for(i in vars){
max <- quantile(X[,i],0.75, na.rm=TRUE) + (IQR(X[,i], na.rm=TRUE) * 1.5 )
min <- quantile(X[,i],0.25, na.rm=TRUE) - (IQR(X[,i], na.rm=TRUE) * 1.5 )
idx <- which(X[,i] < min | X[,i] > max)
print(paste(i, length(idx), sep=''))
Outliers <- c(Outliers, idx)
}
Outliers <- sort(Outliers)
Outliers
X1 <- X[-Outliers,]
X1<-scale(X1)
bootstrapInstability(X1,20)
bootstrapInstability(X1,20, clusterAlg = myHierarchicalClustering, linkage = "single")
bootstrapInstability(X1,20, clusterAlg = myHierarchicalClustering, linkage = "average")
plot(prcomp(X)$x, col = rainbow(3)[y])
plot3D(X1, myHierarchicalClustering(X1,3)$cluster)
y1<-myKmean(X,3)$cluster
y1
y1<-myKmean(X,3)$cluster
y1<-myKmean(X,3)$cluster
y1
X <- scale(X)
y1<-myKmean(X,3)$cluster
y1
adj.rand.index(y,y1)
as.numeric(y)
y1[y1==2]<-4
y1[y1==1]<-2
y1[y1==4]<-1
y1
sum(y==y1)/length(y)
kolesnikov(X1, 20)
sil <- array(0,19)
for (k in 2:20){
print(k)
#cn <- consensusCLustering_Boot(X,k,clustering = myKmean, C=100)
cn <- myHierarchicalClustering(X1,k,linkage = "average")$cluster
sil[k-1]<- mean(silhouette(cn, dist(X1))[,3])
}
print(sil)
plot(2:20, sil)
sil <- array(0,19)
for (k in 2:20){
print(k)
#cn <- consensusCLustering_Boot(X,k,clustering = myKmean, C=100)
cn <- myKmean(X1,k)$cluster
sil[k-1]<- mean(silhouette(cn, dist(X1))[,3])
}
print(sil)
plot(2:20, sil)
db <- array(0,19)
for (k in 2:20){
print(k)
cn <- myKmean(X1,k)$cluster
db[k-1]<- index.DB(X1, cn)$DB
}
print(db)
plot(2:20, db)
c <- array(0,19)
for (k in 2:20){
print(k)
cn <- myKmean(X,k)$cluster
c[k-1]<- ch(X, cn)
}
print(c)
plot(2:20, c)
curvature <- function(X, maxK, clusterAlg = myKmean, verbose=TRUE,...){
J <- array(0, maxK+1)
for (k in 1:(maxK+1)){
cluster.variance <- array(0, 20)
for (t in 1:20){
clustering <- clusterAlg(X,k,...)
cluster.variance[t] <- sum((X - clustering$centers[clustering$cluster,])^2)
}
J[k] <- min(cluster.variance)
}
r <- numeric(maxK-1)
for (k in 2:maxK){
r[k-1] <- abs((J[k-1]-J[k])/(J[k]-J[k+1]))
}
tmp  <- numeric(maxK)
tmp[2:maxK] <- r
r <- tmp
kopt <- which.max(r)
if(verbose){
plot(J)
plot(r)
}
return(list("kopt"=kopt, "J"=J, "r"=r))
}
curvature(X, 20)
curvature(X, 20, clusterAlg = myHierarchicalClustering)
db <- array(0,19)
for (k in 2:20){
print(k)
cn <- myHierarchicalClustering(X1,k)$cluster
db[k-1]<- index.DB(X1, cn)$DB
}
print(db)
plot(2:20, db)
c <- array(0,19)
for (k in 2:20){
print(k)
cn <- myHierarchicalClustering(X,k)$cluster
c[k-1]<- ch(X, cn)
}
print(c)
plot(2:20, c)
curvature(X1, 20)
curvature(X1, 20, clusterAlg = myHierarchicalClustering)
c <- array(0,19)
for (k in 2:20){
print(k)
cn <- myHierarchicalClustering(X1,k)$cluster
c[k-1]<- ch(X1, cn)
}
print(c)
plot(2:20, c)
c <- array(0,19)
for (k in 2:20){
print(k)
cn <- myHierarchicalClustering(X1,k)$cluster
c[k-1]<- calinhara(X1, cn)
}
print(c)
plot(2:20, c)
kolesnikov(X1, 20, myHierarchicalClustering)
sum(y==y1)/length(y)
library(class)
library(mclust)
library(rgl)
library(car)
library(caret)
library(anocva)
library(apcluster)
library(MixAll)
library(fpc)
library(clusterSim)
library(clValid)
library(RSKC)
library(fossil)
library(dendextend)
library(spatstat)
kolesnikov <- function(X, maxK, clusterAlg = myKmean, ...){
X <- as.matrix(X)
W <- array(0, maxK)
for (k in 1:maxK){
clustering <- clusterAlg(X, k, ...)
centers <- clustering$centers
dist <- X - centers[clustering$cluster,]
W[k] <- sum(dist^2)
}
numerator <- maxK*sum(log(1:maxK)^2)-sum(log(1:maxK))^2
denominator <- maxK*sum(log(1:maxK)*log(W)) - sum(log(1:maxK))*sum(log(W))
a <- -2*numerator/denominator
pcf <- W*(1:maxK)^(2/a)
kopt <- which.min(pcf)
return(list("kopt"= kopt,"PCF" = pcf, "a"=a))
}
generateDataset1 <- function(n){
datasets <- array(0, dim = c(n,250,2))
for (i in 1:n){
mu1 <- c(0,0)
mu2 <- c(-2.5,2.5)
mu3 <- c(2.5,2.5)
mu4 <- c(-2.5,-2.5)
mu5 <- c(2.5,-2.5)
c1 <- mvrnorm(n = 50, mu = mu1, Sigma = diag(2))
c2 <- mvrnorm(n = 50, mu = mu2, Sigma = diag(2))
c3 <- mvrnorm(n = 50, mu = mu3, Sigma = diag(2))
c4 <- mvrnorm(n = 50, mu = mu4, Sigma = diag(2))
c5 <- mvrnorm(n = 50, mu = mu5, Sigma = diag(2))
datasets[i, ,] <- rbind(c1,c2,c3,c4,c5)
}
datasets
}
generateDataset2 <- function(n){
datasets <- array(0, dim = c(n, 250,2))
for (i in 1:n){
mu1 <- c(0,0)
mu2 <- c(-2.5,2.5)
mu3 <- c(2.5,2.5)
mu4 <- c(-2.5,-2.5)
mu5 <- c(2.5,-2.5)
sigma <- rbind(c(16.5,13),c(11,11))
sigma <- sigma/norm(sigma)
c1 <- mvrnorm(n = 50, mu = mu1, Sigma = sigma)
c2 <- mvrnorm(n = 50, mu = mu2, Sigma = sigma)
c3 <- mvrnorm(n = 50, mu = mu3, Sigma = sigma)
c4 <- mvrnorm(n = 50, mu = mu4, Sigma = sigma)
c5 <- mvrnorm(n = 50, mu = mu5, Sigma = sigma)
datasets[i, ,] <- rbind(c1,c2,c3,c4,c5)
}
datasets
}
generateDataset3 <- function(n){
datasets <- array(0, dim = c(n, 250,2))
for (i in 1:n){
mu1 <- c(0,0)
mu2 <- c(-2.5,2.5)
mu3 <- c(2.5,2.5)
mu4 <- c(-2.5,-2.5)
mu5 <- c(2.5,-2.5)
sigma1 <- rbind(c(16.5,13),c(11,11))
sigma1 <- sigma1/norm(sigma1)
sigma2 <- rbind(c(4.5,-1),c(-1, 3))
sigma2 <- sigma2/norm(sigma2)
c1 <- mvrnorm(n = 50, mu = mu1, Sigma = diag(2))
c2 <- mvrnorm(n = 50, mu = mu2, Sigma = sigma1)
c3 <- mvrnorm(n = 50, mu = mu3, Sigma = sigma2)
c4 <- mvrnorm(n = 50, mu = mu4, Sigma = sigma2)
c5 <- mvrnorm(n = 50, mu = mu5, Sigma = sigma1)
datasets[i, ,] <- rbind(c1,c2,c3,c4,c5)
}
datasets
}
generateDataset4 <- function(n){
datasets <- array(0, dim = c(n, 250,10))
for (i in 1:n){
mu1 <- c(0,0,0,0,0,0,0,0,1.6,0)
mu2 <- c(-1.6,1.6,0,0,0,0,1.6,0,0,0)
mu3 <- c(1.6,1.6,0,1.6,0,0,0,0,0,0)
mu4 <- c(-1.6,-1.6,0,0,0,1.6,0,0,-1.6,0)
mu5 <- c(1.6,-1.6,0,0,0,1.6,0,0,0,0)
c1 <- mvrnorm(n = 50, mu = mu1, Sigma = diag(10))
c2 <- mvrnorm(n = 50, mu = mu2, Sigma = diag(10))
c3 <- mvrnorm(n = 50, mu = mu3, Sigma = diag(10))
c4 <- mvrnorm(n = 50, mu = mu4, Sigma = diag(10))
c5 <- mvrnorm(n = 50, mu = mu5, Sigma = diag(10))
datasets[i, ,] <- rbind(c1,c2,c3,c4,c5)
}
datasets
}
generateDataset5 <- function(n){
datasets <- array(0, dim = c(n, 200,2))
for (i in 1:n){
mu2 <- c(-2.5,2.5)
mu3 <- c(2.5,2.5)
mu4 <- c(-2.5,-2.5)
mu5 <- c(2.5,-2.5)
c2 <- rexp(n = 100, rate = 1)
c2 <- t(t(matrix(c2, nrow = 50, byrow = TRUE)) + mu2)
c2[,2] <- max(c2[,2]) - c2[,2] + min(c2[,2])
c3 <- rexp(n = 100, rate = 1)
c3 <- t(t(matrix(c3, nrow = 50, byrow = TRUE)) + mu3)
c3 <- t(apply(c3, 2, max) - t(c3) + apply(c3, 2, min))
c4 <- rexp(n = 100, rate = 1)
c4 <- t(t(matrix(c4, nrow = 50, byrow = TRUE)) + mu4)
c5 <- rexp(n = 100, rate = 1)
c5 <- t(t(matrix(c5, nrow = 50, byrow = TRUE)) + mu5)
c5[,1] <- max(c5[,1]) - c5[,1] + min(c5[,1])
datasets[i, ,] <- rbind(c2,c3,c4,c5)
}
datasets
}
twoMoons <- function(N){
moon1 <- array(0, dim = c(N,2))
r1 <- runif(N, 5,6)
r2 <- runif(N,5,6)
alpha1 <- runif(N,0,1)
alpha2 <- runif(N,-1,0)
moon1 <- cbind(3 + (r1*cospi(alpha1)), r1*sinpi(alpha1))
moon2 <- cbind(-3 + (r2*cospi(alpha2)), 3 + r2*sinpi(alpha2))
X <- rbind(moon1,moon2)
return(X)
}
generateDataset6 <- function(n){
datasets <- array(0, dim = c(n, 200,2))
for (i in 1:n){
datasets[i, ,] <- mvrnorm(200, mu = c(0,0), Sigma = diag(2))
}
datasets
}
data6 <- generateDataset6(1)[1,,]
plot(data6)
#' K-means clustering
#'
#' This function is a wrapper to \code{\link[stats:kmeans]{kmeans}} in the package stats.
#'
#' @param X data matrix or data frame of size n x d, n observations and d features
#' @param k number of clusters
#'
#' @importFrom stats kmeans
#' @return list of 3 components:
#' \describe{
#' \item{\preformatted{cluster}}{vector of integer between 1 and k containing the allocation of each point}
#' \item{\preformatted{centers}}{matrix (d x k) of the centers of each cluster}
#' \item{\preformatted{predict}}{function predicting to which cluster an observation belongs. The input can be a single observation vector or a matrix of several observations. The assigned cluster is the one with the closest center to the given observation.}
#' }
#' @export
#'
myKmean <- function(X, k){
res <- kmeans(X,k, iter.max = 50)
list("cluster" = res$cluster, "predict" = function(Z){
closest.point <- function(x) {
cluster.dist <- apply(res$centers, 1, function(y) sqrt(sum((x-y)^2)))
return(which.min(cluster.dist)[1])}
return(apply(Z, 1, closest.point))
}, "centers" = res$centers)
}
#' Spectral clustering
#'
#' This function is a wrapper to \code{\link[anocva:spectralClustering]{spectralClustering}} in the package anocva.
#'
#' @param X data matrix or data frame of size n x d, n observations and d features
#' @param k number of clusters
#' @param simi similarity measure between two vectors
#'
#' @importFrom anocva spectralClustering
#' @importFrom class knn
#' @return list of 3 components:
#' \describe{
#' \item{\preformatted{cluster}}{vector of integer between 1 and k containing the allocation of each point}
#' \item{\preformatted{centers}}{matrix (d x k) of the centers of each cluster}
#' \item{\preformatted{predict}}{function predicting to which cluster an observation belongs. The prediciton is done with the k-nearest neighbours function.}
#' }
#' @export
#'
mySpectralClustering <- function(X, k, simi = expSimMat, ...){
A <- simi(X)
cluster <- spectralClustering(A, k, ...)
d <- dim(X)[2]
centers <- aggregate(X, rep(list(cluster),d), mean)[,-(1:d)]
prediction <- function(x){
knn(X,x,cluster,k=10)
}
return(list("cluster" = cluster, "predict" = prediction, "centers"=centers))
}
#' Hierarchical clustering
#'
#' This function performs hierarchical clustering in either average, complete or single linkage.
#'
#' @param X data matrix or data frame of size n x d, n observations and d features
#' @param k number of clusters
#' @param distance character for the type of distance. Default is "euclidean"
#' @param linkage "single", "average" or "complete"
#'
#' @import stats
#' @importFrom class knn
#' @return list of 3 components:
#' \describe{
#' \item{\preformatted{cluster}}{vector of integer between 1 and k containing the allocation of each point}
#' \item{\preformatted{centers}}{matrix (d x k) of the centers of each cluster}
#' \item{\preformatted{predict}}{function predicting to which cluster an observation belongs. The prediciton is done with the k-nearest neighbours function.}
#' }
#' @export
#'
myHierarchicalClustering <- function(X,k, distance = "euclidean", linkage = "average"){
dist_M <- dist(X, method = distance)
h_clust <- hclust(dist_M, method = linkage)
cluster <- cutree(h_clust, k=k)
d <- dim(X)[2]
centers <- aggregate(X, rep(list(cluster),d), mean)[,-(1:d)]
prediction <- function(x){
knn(X,x,cluster,k=10)
}
return(list("cluster" = cluster, "predict" = prediction, "centers"=centers))
}
#' Partitioning around medoids
#'
#' This function is a wrapper to \code{\link[cluster:pam]{pam}} in the package cluster.
#'
#' @param X data matrix or data frame of size n x d, n observations and d features
#' @param k number of clusters
#' @param ... additional parameter for pam function
#'
#' @importFrom cluster pam
#' @importFrom class knn
#' @return list of 3 components:
#' \describe{
#' \item{\preformatted{cluster}}{vector of integer between 1 and k containing the allocation of each point}
#' \item{\preformatted{centers}}{matrix (d x k) of the centers of each cluster}
#' \item{\preformatted{predict}}{function predicting to which cluster an observation belongs. The prediciton is done with the k-nearest neighbours function.}
#' }
#' @export
#'
myPam <- function(X,k, ...){
pa <- pam(X,k,...)
prediction <- function(x){
knn(X,x,pa$clustering, k=10)
}
return(list("cluster"=pa$clustering, "centers"=pa$medoids, "predict"= prediction))
}
myEM <- function(X,k, ...){
em <- Mclust(X,k, verbose = F, ...)
prediction <- function(x){
predict.Mclust(em, x)$classification
}
return(list("cluster"=em$classification, "centers"=t(em$parameters$mean), "predict"= prediction))
}
#' @references Kolesnikov, A., Trichina, E., and Kauranne, T. (2015). Estimating the number of clusters in a numerical data set via quantizationerror modeling.Pattern Recognition, 48:941-952.
kolesnikov <- function(X, maxK, clusterAlg = myKmean, ...){
X <- as.matrix(X)
W <- array(0, maxK)
for (k in 1:maxK){
clustering <- clusterAlg(X, k, ...)
centers <- clustering$centers
dist <- X - centers[clustering$cluster,]
W[k] <- sum(dist^2)
}
numerator <- maxK*sum(log(1:maxK)^2)-sum(log(1:maxK))^2
denominator <- maxK*sum(log(1:maxK)*log(W)) - sum(log(1:maxK))*sum(log(W))
a <- -2*numerator/denominator
pcf <- W*(1:maxK)^(2/a)
kopt <- which.min(pcf)
return(list("kopt"= kopt,"PCF" = pcf, "a"=a))
}
X <- generateDataset1(1,,)[1,,]
X <- generateDataset1(1)[1,,]
res <- kolesnikov(X, 10)
res
a <- res$a
W <- res$PCF * (1:10)^(-2/a)
d <- W^(-a/2)
j <- d[-1] - d[-length(d)]
plot(j)
plot(W)
d <- W^(-2/2)
j <- d[-1] - d[-length(d)]
plot(j)
d[1]
X <- generateDataset5(1)[1,,]
dim(X)
X <- generateDataset4(1)[1,,]
dim(X)
res <- kolesnikov(X, 20)
a <- res$a
res
W <- res$PCF * (1:10)^(-2/a)
d <- W^(-a/2)
j <- d[-1] - d[-length(d)]
plot(j)
plot(d)
a
plot(W)
res
W <- res$PCF * (1:20)^(-2/a)
d <- W^(-a/2)
j <- d[-1] - d[-length(d)]
plot(j)
plot(W)
d <- W^(-10/2)
j <- d[-1] - d[-length(d)]
plot(j)
