Wb[k,b] <- sum(aggregate(idx, list(cluster), function(i)sum(dist(Xb[i,]))/length(i))[-1])
}
}
lbar <- apply(log(Wb), 1, mean)
gap <- lbar - log(W)
sdk <- sqrt(apply((log(Wb)-lbar)^2, 1, mean))
s <- sdk * sqrt(1+1/B)
kopt <- which(gap[-(maxK+1)] >= gap[-1]-s[-1])[1]
if(length(kopt)==0){
kopt <- 1
}
plot(gap, main = "Gap statistics with error bars", xlab = "number of clusters", ylab = "Gap", type = "b")
arrows(1:(maxK+1), gap+s, 1:(maxK+1), gap-s, angle = 90, code = 3, length = 1/8)
return(list(kopt=kopt, gap = gap, s = s))
}
Gap(X, 10, B = 200)
Gap <- function(X, maxK, clusterAlg = myKmean, B = 50, null_hypothesis = "gaussian", verbose = TRUE, ...){
X <- as.matrix(X)
W <- numeric(maxK+1)
n <- nrow(X)
W[1] <- sum(dist(X))/n
if(verbose){
print("Original dataset")
}
for (k in 2:(maxK+1)){
cluster <- clusterAlg(X, k, ...)$cluster
idx <- 1:n
W[k] <- sum(aggregate(idx, list(cluster), function(i)sum(dist(X[i,]))/length(i))[-1])
}
Wb <- matrix(0, maxK+1, B)
switch(null_hypothesis,
gaussian = {
param <- rbind(apply(X, 2, mean), apply(X,2,var))
distrib <- mvrnorm
},
uniformity = {
Xtmp <- scale(X, center = TRUE, scale = FALSE)
v <- svd(Xtmp)$v
Xtmp <- Xtmp %*% v
param <- apply(Xtmp, 2, range)
distrib <- runif
},
uniform = {
param <- apply(X, 2, range)
distrib <- runif
})
for (b in 1:B){
if(verbose){
print(paste("Reference dataset number: ", b))
}
Xb <- apply(param, 2, function(p) distrib(n=n, p[1], p[2]))
if (null_hypothesis == "uniformity"){
Xb <- Xb %*% t(v)
}
Wb[1,b] <- sum(dist(Xb))/n
for (k in 2:(maxK+1)){
cluster <- clusterAlg(Xb, k, ...)$cluster
idx <- 1:n
Wb[k,b] <- sum(aggregate(idx, list(cluster), function(i)sum(dist(Xb[i,]))/length(i))[-1])
}
}
lbar <- apply(log(Wb), 1, mean)
gap <- lbar - log(W)
sdk <- sqrt(apply((log(Wb)-lbar)^2, 1, mean))
s <- sdk * sqrt(1+1/B)
kopt <- which(gap[-(maxK+1)] >= gap[-1]-s[-1])[1]
if(length(kopt)==0){
kopt <- 1
}
plot(gap, main = "Gap statistics with error bars", xlab = "number of clusters", ylab = "Gap", type = "b")
arrows(1:(maxK+1), gap+s, 1:(maxK+1), gap-s, angle = 90, code = 3, length = 1/8)
return(list(kopt=kopt, gap = gap, s = s))
}
Gap(X, 10, B = 200)
Gap(X, 10, B = 200, myHierarchicalClustering, linkage="single")
Gap(X, 10, B = 200, myHierarchicalClustering, linkage="complete")
Gap(X, 10, B = 200, myHierarchicalClustering, linkage="average")
Gap(X, 10, B = 200, myPam)
library(Rdpack)
viewRd("man/Gap.Rd", type="html")
viewRd("man/Gap.Rd", type="html")
viewRd("man/clest.Rd", type="html")
roxygen2::roxygenise()
viewRd("man/Gap.Rd", type="html")
roxygen2::roxygenise()
viewRd("man/Gap.Rd", type="html")
a <- list()
a[["ok"]]
names(a) <- c("a","b","c")
a <- list(length = 3)
names(a) <- c("a","b","c")
names(a) <- c("d","b","c")
a <- list(type = "list",length = 3)
names(a) <- c("d","b","c")
a
a <- vector(type = "list",length = 3)
a <- vector(mode = "list",length = 3)
names(a) <- c("d","b","c")
a[c("d","c")]
a["c"] <- 0
a["d"] <- 1
a["b"] <- 2
a[c("d","c")]
a[c("d","c", "dsd")]
a
class(a["d"])
class(a[["d"]])
a[[c("d","c", "dsd")]]
a[[c("d","c")]]
a[c("d","c", "dsd")] == 0
s <- a[c("d","c", "dsd")]
s
class(s)
s[[1]]
length(s)
foo <- c(1,2,3)
names(foo) <- c("a","b","c")
foo["a"]
class(foo["a"])
foo[c("a","b")]
foo[c("a","b")][2]
class(foo[c("a","b")][2])
class(foo[c("a","b")])
d <- foo[c("a","b")]
names(foo)[d]
a <- as.character(1:10)
a
a == c("2","6","1")
dunn
cluster.stats
bootstrapInstability <- function(X, maxK, B = 50, clusterAlg = myKmean, similarity = adj.rand.index, verbose =TRUE,...){
n <- dim(X)[1]
instability <- matrix(0, maxK-1, B)
cluster1 <- cluster2 <- numeric(n)
for (b in 1:B){
if(verbose==TRUE){
print(paste("Iteration: ", b))
}
idx1 <- sample(1:n,n,replace = TRUE)
idx2 <- sample(1:n,n,replace = TRUE)
inv_idx1 <- setdiff(1:n,idx1)
inv_idx2 <- setdiff(1:n,idx2)
X1 <- X[idx1,]
X2 <- X[idx2,]
for (k in 2:maxK){
X1.cluster <- clusterAlg(X1, k,...)
X2.cluster <- clusterAlg(X2, k,...)
cluster1[idx1] <- X1.cluster$cluster
cluster2[idx2] <- X2.cluster$cluster
cluster1[inv_idx1] <- X1.cluster$predict(X[inv_idx1,])
cluster2[inv_idx2] <- X2.cluster$predict(X[inv_idx2,])
instability[k-1,b] <- 1 - similarity(cluster1, cluster2)
}
}
inst_mean <- apply(instability,1,mean)
plot(2:maxK, inst_mean, main = "Instability", xlab = "number of clusters", ylab = "instability")
return(list("inst_mean"= inst_mean, "kopt"=which.min(inst_mean)+1, "instability"=instability))
}
bootstrapInstability(X, 10)
bootstrapInstability <- function(X, maxK, B = 50, clusterAlg = myKmean, similarity = adj.rand.index, verbose =TRUE,...){
n <- dim(X)[1]
instability <- matrix(0, maxK-1, B)
cluster1 <- cluster2 <- numeric(n)
for (b in 1:B){
if(verbose==TRUE){
print(paste("Iteration: ", b))
}
idx1 <- sample(1:n,n,replace = TRUE)
idx2 <- sample(1:n,n,replace = TRUE)
inv_idx1 <- setdiff(1:n,idx1)
inv_idx2 <- setdiff(1:n,idx2)
X1 <- X[idx1,]
X2 <- X[idx2,]
for (k in 2:maxK){
X1.cluster <- clusterAlg(X1, k,...)
X2.cluster <- clusterAlg(X2, k,...)
cluster1[idx1] <- X1.cluster$cluster
cluster2[idx2] <- X2.cluster$cluster
cluster1[inv_idx1] <- X1.cluster$predict(X[inv_idx1,])
cluster2[inv_idx2] <- X2.cluster$predict(X[inv_idx2,])
instability[k-1,b] <- 1 - similarity(cluster1, cluster2)
}
}
inst_mean <- apply(instability,1,mean)
plot(2:maxK, inst_mean, main = "Instability", xlab = "number of clusters", ylab = "instability", type = "b")
return(list("inst_mean"= inst_mean, "kopt"=which.min(inst_mean)+1, "instability"=instability))
}
bootstrapInstability(X, 10)
roxygen2::roxygenise()
library(class)
library(mclust)
library(rgl)
library(car)
library(caret)
library(anocva)
library(apcluster)
library(MixAll)
library(fpc)
library(clusterSim)
library(clValid)
library(RSKC)
library(fossil)
library(dendextend)
library(spatstat)
generateDataset1 <- function(n){
datasets <- array(0, dim = c(n,250,2))
for (i in 1:n){
mu1 <- c(0,0)
mu2 <- c(-2.5,2.5)
mu3 <- c(2.5,2.5)
mu4 <- c(-2.5,-2.5)
mu5 <- c(2.5,-2.5)
c1 <- mvrnorm(n = 50, mu = mu1, Sigma = diag(2))
c2 <- mvrnorm(n = 50, mu = mu2, Sigma = diag(2))
c3 <- mvrnorm(n = 50, mu = mu3, Sigma = diag(2))
c4 <- mvrnorm(n = 50, mu = mu4, Sigma = diag(2))
c5 <- mvrnorm(n = 50, mu = mu5, Sigma = diag(2))
datasets[i, ,] <- rbind(c1,c2,c3,c4,c5)
}
datasets
}
X <- generateDataset1(1)[1,,]
#' K-means clustering
#'
#' This function is a wrapper to \code{\link[stats:kmeans]{kmeans}} in the package stats.
#'
#' @param X data matrix or data frame of size n x d, n observations and d features
#' @param k number of clusters
#'
#' @importFrom stats kmeans
#' @return list of 3 components:
#' \describe{
#' \item{\preformatted{cluster}}{vector of integer between 1 and k containing the allocation of each point}
#' \item{\preformatted{centers}}{matrix (d x k) of the centers of each cluster}
#' \item{\preformatted{predict}}{function predicting to which cluster an observation belongs. The input can be a single observation vector or a matrix of several observations. The assigned cluster is the one with the closest center to the given observation.}
#' }
#' @export
#'
myKmean <- function(X, k){
res <- kmeans(X,k, iter.max = 50)
list("cluster" = res$cluster, "predict" = function(Z){
closest.point <- function(x) {
cluster.dist <- apply(res$centers, 1, function(y) sqrt(sum((x-y)^2)))
return(which.min(cluster.dist)[1])}
return(apply(Z, 1, closest.point))
}, "centroids" = res$centers)
}
#' Spectral clustering
#'
#' This function is a wrapper to \code{\link[anocva:spectralClustering]{spectralClustering}} in the package anocva.
#'
#' @param X data matrix or data frame of size n x d, n observations and d features
#' @param k number of clusters
#' @param simi similarity measure between two vectors
#'
#' @importFrom anocva spectralClustering
#' @importFrom class knn
#' @return list of 3 components:
#' \describe{
#' \item{\preformatted{cluster}}{vector of integer between 1 and k containing the allocation of each point}
#' \item{\preformatted{centers}}{matrix (d x k) of the centers of each cluster}
#' \item{\preformatted{predict}}{function predicting to which cluster an observation belongs. The prediciton is done with the k-nearest neighbours function.}
#' }
#' @export
#'
mySpectralClustering <- function(X, k, simi){
A <- simi(X)
cluster <- spectralClustering(A,k)
d <- dim(X)[2]
centers <- aggregate(X, rep(list(cluster),d), mean)[,-(1:d)]
prediction <- function(x){
knn(X,x,cluster,k=10)
}
return(list("cluster" = cluster, "predict" = prediction, "centers"=centers))
}
#' Hierarchical clustering
#'
#' This function performs hierarchical clustering in either average, complete or single linkage.
#'
#' @param X data matrix or data frame of size n x d, n observations and d features
#' @param k number of clusters
#' @param distance character for the type of distance. Default is "euclidean"
#' @param linkage "single", "average" or "complete"
#'
#' @import stats
#' @importFrom class knn
#' @return list of 3 components:
#' \describe{
#' \item{\preformatted{cluster}}{vector of integer between 1 and k containing the allocation of each point}
#' \item{\preformatted{centers}}{matrix (d x k) of the centers of each cluster}
#' \item{\preformatted{predict}}{function predicting to which cluster an observation belongs. The prediciton is done with the k-nearest neighbours function.}
#' }
#' @export
#'
myHierarchicalClustering <- function(X,k, distance = "euclidean", linkage = "average"){
dist_M <- dist(X, method = distance)
h_clust <- hclust(dist_M, method = linkage)
cluster <- cutree(h_clust, k=k)
d <- dim(X)[2]
centers <- aggregate(X, rep(list(cluster),d), mean)[,-(1:d)]
prediction <- function(x){
knn(X,x,cluster,k=10)
}
return(list("cluster" = cluster, "predict" = prediction, "centers"=centers))
}
#' Partitioning around medoids
#'
#' This function is a wrapper to \code{\link[cluster:pam]{pam}} in the package cluster.
#'
#' @param X data matrix or data frame of size n x d, n observations and d features
#' @param k number of clusters
#' @param ... additional parameter for pam function
#'
#' @importFrom cluster pam
#' @importFrom class knn
#' @return list of 3 components:
#' \describe{
#' \item{\preformatted{cluster}}{vector of integer between 1 and k containing the allocation of each point}
#' \item{\preformatted{centers}}{matrix (d x k) of the centers of each cluster}
#' \item{\preformatted{predict}}{function predicting to which cluster an observation belongs. The prediciton is done with the k-nearest neighbours function.}
#' }
#' @export
#'
myPam <- function(X,k, ...){
pa <- pam(X,k,...)
prediction <- function(x){
knn(X,x,pa$clustering, k=10)
}
return(list("cluster"=pa$clustering, "centers"=pa$medoids, "predict"= prediction))
}
cl <- myKmean(X, 5)
cl <- myKmean(X, 5)$cluster
aggregate(1:nrow(X), list(cl), function(i) var(X[i,]))[-1]
var(X)
aggregate(1:nrow(X), list(cl), function(i) sum((X[i,] - apply(X[i,], 2, mean))^2 / length(i)))[-1]
plot(X, col = rainbow(5)[cl])
aggregate(1:nrow(X), list(cl), function(i) sum((X[i,] - apply(X[i,], 2, mean))^2 / length(i)))[-1]
cl
aggregate(1:nrow(X), list(cl), function(i){sum((X[i,] - apply(X[i,], 2, mean))^2 / length(i))})[-1]
rainbow(5)
X[cl == 2]
sum((X[cl==2,] - apply(X[cl==2,], 2, mean))^2) / sum(cl==2)
sum((X[cl==1,] - apply(X[cl==1,], 2, mean))^2) / sum(cl==1)
sum((X[cl==4,] - apply(X[cl==4,], 2, mean))^2) / sum(cl==4)
1 - length(cl)==1
1 - (length(cl)==1)
cl = 1
(1 - (length(cl)==1))*sum((X[cl,]-apply(X[cl,],2,mean))^2)/length(cl)
is.matrix(X[cl,])
sum(aggregate(1:250, list(cl), function(i) {if(is.matrix(X[i,]) return(sum((X[i,] - apply(X[i,], 2, mean))^2) / length(i)) else return(0)})[-1])
ch <- function(X, cluster){
idx <- 1:nrow(X)
W <- sum(aggregate(idx, list(cluster), function(i) {if(is.matrix(X[i,]) {return(sum((X[i,] - apply(X[i,], 2, mean))^2) / length(i))} else {return(0)}})[-1])
W
}
ch <- function(X, cluster){
idx <- 1:nrow(X)
W <- sum(aggregate(idx, list(cluster), function(i) {if(is.matrix(X[i,]){return(sum((X[i,] - apply(X[i,], 2, mean))^2) / length(i))}; else {return(0)}})[-1])
W
}
ch <- function(X, cluster){
idx <- 1:nrow(X)
W <- sum(aggregate(idx, list(cluster), function(i) {if(is.matrix(X[i,])){return(sum((X[i,] - apply(X[i,], 2, mean))^2) / length(i))}; else {return(0)}})[-1])
W
}
ch <- function(X, cluster){
idx <- 1:nrow(X)
W <- sum(aggregate(idx, list(cluster), function(i){if(is.matrix(X[i,])){return(sum((X[i,] - apply(X[i,], 2, mean))^2) / length(i))} else {return(0)}})[-1])
W
}
ch(X,cl)
length(cl)
cl <- myKmean(X,5)$cluster
ch(X,cl)
cl[1] <- 6
ch(X,cl)
ch <- function(X, cluster){
n <- nrow(X)
k <- length(table(cluster))
W <- sum(aggregate(1:n, list(cluster), function(i){if(is.matrix(X[i,])){return(sum((X[i,] - apply(X[i,], 2, mean))^2) / length(i))} else {return(0)}})[-1])
B <- sum(aggregate(1:n, list(cluster), function(i){if(is.matrix(X[i,])){return((apply(X,2,mean) - apply(X[i,], 2, mean))^2) * length(i))} else {return(sum((X[i,]-apply(X,2,mean))^2}})[-1])
return(B * (n-k)/(W * (k-1)))
}
ch <- function(X, cluster){
n <- nrow(X)
k <- length(table(cluster))
W <- sum(aggregate(1:n, list(cluster), function(i){if(is.matrix(X[i,])){return(sum((X[i,] - apply(X[i,], 2, mean))^2) / length(i))} else {return(0)}})[-1])
B <- sum(aggregate(1:n, list(cluster), function(i){if(is.matrix(X[i,])){return(sum((apply(X,2,mean) - apply(X[i,], 2, mean))^2) * length(i))} else {return(sum((X[i,]-apply(X,2,mean))^2))}})[-1])
return(B * (n-k)/(W * (k-1)))
}
ch(X,cluster)
ch(X,cl)
calinhara(X, cl)
cl <- myKmean(X,5)$cluster
calinhara(X, cl)
ch(X,cl)
calinhara
class(X)
X <- rbind(c(0,1),c(2,3),c(4,5))
cl <- c(1,2,1)
X <- rbind(c(0,1),c(2,3),c(4,5), c(6))
X <- rbind(c(0,1),c(2,3),c(4,5), c(6,7))
X
cl <- c(1,2,1,2)
calinhara(X, cl)
ch(X,cl)
apply(X, 2, mean)
apply(X[cl ==2], 2, mean)
apply(X[cl ==2,], 2, mean)
apply(X[cl ==1,], 2, mean)
X[cl==1,] - apply(X[cl==1,],2,mean)
X[cl==1]
X[cl==1,]
apply(X[cl ==1,], 2, mean)
ch <- function(X, cluster){
n <- nrow(X)
k <- length(table(cluster))
W <- sum(aggregate(1:n, list(cluster), function(i){if(is.matrix(X[i,])){return(sum((t(X[i,]) - apply(X[i,], 2, mean))^2) / length(i))} else {return(0)}})[-1])
B <- sum(aggregate(1:n, list(cluster), function(i){if(is.matrix(X[i,])){return(sum((apply(X,2,mean) - apply(X[i,], 2, mean))^2) * length(i))} else {return(sum((X[i,]-apply(X,2,mean))^2))}})[-1])
return(B * (n-k)/(W * (k-1)))
}
ch(X,cl)
t(X[cl==1,]) - apply(X[cl==1,],2,mean)
t(X[cl==2,]) - apply(X[cl==1,],2,mean)
t(X[cl==2,]) - apply(X[cl==2,],2,mean)
sum((t(X[cl==2,]) - apply(X[cl==2,],2,mean))^2)/2
apply(X,2,mean) - apply(X[1,], 2, mean)
apply(X,2,mean) - apply(X[cl==1,], 2, mean)
X <- generateDataset1(1)[1,,]
cl <- myKmean(X,5$)
cl <- myKmean(X,5)$cluster
ch(X,cl)
c <- array(0,19)
for (k in 2:20){
print(k)
cn <- myKmean(X,k, clustering = myKmean)
c[k-1]<- ch(X, cn)
}
print(c)
plot(2:20, c)
c <- array(0,19)
for (k in 2:20){
print(k)
cn <- myKmean(X,k)
c[k-1]<- ch(X, cn)
}
print(c)
plot(2:20, c)
c <- array(0,19)
for (k in 2:20){
print(k)
cn <- myKmean(X,k)$cluster
c[k-1]<- ch(X, cn)
}
print(c)
plot(2:20, c)
library(MASS)
library(clusterGeneration)
library(cluster)
library(gtools)
library(ramify)
library(MLmetrics)
library(caret)
library(mlbench)
library(SwarmSVM)
library(EMCluster)
library(mclust)
mu1 <- runif(2, min = -20, max = 20)
mu2 <- runif(2, min = -20, max = 20)
mu3 <- runif(2, min = -20, max = 20)
mu4 <- runif(2, min = -20, max = 20)
mu1 <- c(0,10)
mu2 <- c(10,10)
mu3 <- c(10,0)
mu4 <- c(0,0)
sig1 <- genPositiveDefMat(2)$Sigma/2
sig2 <- genPositiveDefMat(2)$Sigma/2
sig3 <- genPositiveDefMat(2)$Sigma/2
sig4 <- genPositiveDefMat(2)$Sigma/2
#sig1 = array(c(10,0,0,1), dim=c(2,2))
#sig2 = array(c(1,0,0,10), dim=c(2,2))
c1 <- mvrnorm(n = 50, mu = mu1, Sigma = sig1)
c2 <- mvrnorm(n = 50, mu = mu2, Sigma = sig2)
c3 <- mvrnorm(n = 50, mu = mu3, Sigma = sig3)
c4 <- mvrnorm(n = 50, mu = mu4, Sigma = sig4)
X <- rbind(c1,c2,c3,c4)
X <- X[sample.int(nrow(X)),]
plot(c1[,1],c1[,2], col="black", xlim = c(-25,25), ylim = c(-25,15))
points(c2[,1],c2[,2], col="blue")
points(c3[,1],c3[,2], col="red")
points(c4[,1], c4[,2], col="green")
c <- array(0,19)
for (k in 2:20){
print(k)
cn <- myKmean(X,k)$cluster
c[k-1]<- ch(X, cn)
}
print(c)
plot(2:20, c)
X <- scale(X)
c <- array(0,19)
for (k in 2:20){
print(k)
cn <- myKmean(X,k)$cluster
c[k-1]<- ch(X, cn)
}
print(c)
plot(2:20, c)
plot(X)
plot(X, col = rainbow(4)[cl])
cl <- myKmean(X, 4)$cluster
plot(X, col = rainbow(4)[cl])
